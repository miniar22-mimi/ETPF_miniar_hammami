{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miniar22-mimi/ETPF_miniar_hammami/blob/main/Copie_de_Projet_Chatbot_V0_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb4185b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb4185b9",
        "outputId": "fc9e09d9-26b9-4c5c-9306-393e3a76d7a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.7/437.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installation des bibliothèques nécessaires\n",
        "!pip install langchain langchain-community pypdf faiss-cpu -q\n",
        "# or faiss-gpu if you have CUDA\n",
        "!pip install langchain_groq -q\n",
        "!pip install -U langchain-huggingface -q\n",
        "!pip install gradio -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de93cc8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "de93cc8e",
        "outputId": "e66dd604-48c1-472c-adb1-9da8185bc9c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Création d'une nouvelle base FAISS...\n",
            "Erreur lors de la création de la base vectorielle: File path Cosmetiques_2025.pdf is not a valid file or url\n",
            "Nombre de documents indexés : 2\n",
            "Historique chargé: 1 messages\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ce3185b2a0de366a2e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ce3185b2a0de366a2e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "import gradio as gr\n",
        "import os\n",
        "import sys\n",
        "import io\n",
        "\n",
        "# Configuration de l'API Groq\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "# Fonction pour créer une nouvelle base vectorielle\n",
        "def create_new_vectorstore(pdf_path, embedding_function, persist_directory):\n",
        "    try:\n",
        "        # Chargement du PDF\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        pages = loader.load()\n",
        "\n",
        "        # Découpage des documents en chunks\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=200,\n",
        "            chunk_overlap=50,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "        docs = text_splitter.split_documents(pages)\n",
        "\n",
        "        # Création de la base vectorielle\n",
        "        vectorstore = FAISS.from_documents(docs, embedding_function)\n",
        "\n",
        "        # Sauvegarde de la base\n",
        "        if persist_directory:\n",
        "            try:\n",
        "                vectorstore.save_local(persist_directory)\n",
        "                print(f\"Base vectorielle sauvegardée dans {persist_directory}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur lors de la sauvegarde de la base FAISS: {str(e)}\")\n",
        "\n",
        "        return vectorstore\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de la création de la base vectorielle: {str(e)}\")\n",
        "        # Créer une base vectorielle avec des données minimales en cas d'échec\n",
        "        return FAISS.from_texts(\n",
        "            [\"Tendances mode hommes: vêtements oversize, couleurs neutres.\",\n",
        "             \"Tendances mode femmes: années 90, coupes fluides, couleurs vives.\"],\n",
        "            embedding_function\n",
        "        )\n",
        "\n",
        "# Fonction pour charger ou créer la base vectorielle FAISS\n",
        "def setup_vectorstore():\n",
        "    # Chemin vers votre PDF\n",
        "    pdf_path = \"Cosmetiques_2025.pdf\"\n",
        "    persist_directory = \"./faiss_db\"\n",
        "\n",
        "    # Configuration des embeddings HuggingFace\n",
        "    embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "    # Vérifier si la base FAISS existe déjà\n",
        "    if os.path.exists(persist_directory):\n",
        "        print(\"Chargement de la base FAISS existante...\")\n",
        "        try:\n",
        "            vectorstore = FAISS.load_local(persist_directory, embedding_function, allow_dangerous_deserialization=True)\n",
        "            print(\"Base FAISS chargée avec succès\")\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur lors du chargement de la base FAISS: {str(e)}\")\n",
        "            print(\"Création d'une nouvelle base FAISS...\")\n",
        "            vectorstore = create_new_vectorstore(pdf_path, embedding_function, persist_directory)\n",
        "    else:\n",
        "        print(\"Création d'une nouvelle base FAISS...\")\n",
        "        vectorstore = create_new_vectorstore(pdf_path, embedding_function, persist_directory)\n",
        "\n",
        "    print(f\"Nombre de documents indexés : {vectorstore.index.ntotal}\")\n",
        "    return vectorstore\n",
        "\n",
        "# Initialisation du modèle Groq\n",
        "groq_llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\")\n",
        "\n",
        "# Chargement ou création de la base vectorielle\n",
        "try:\n",
        "    vectorstore = setup_vectorstore()\n",
        "except Exception as e:\n",
        "    print(f\"Erreur critique lors de la configuration de la base vectorielle: {str(e)}\")\n",
        "    # Créer une base vectorielle vide ou avec des données minimales en cas d'échec\n",
        "    embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vectorstore = FAISS.from_texts(\n",
        "        [\"Crème hydratante jour/nuit adaptée aux peaux sèches.\",\n",
        "         \"Sérum à la vitamine C pour un teint éclatant.\"],\n",
        "        embedding_function\n",
        "    )\n",
        "\n",
        "# Fonction pour sauvegarder l'historique des conversations\n",
        "def save_conversation_history(history):\n",
        "    try:\n",
        "        with open(\"conversation_history.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            for msg in history:\n",
        "                role = \"Human\" if isinstance(msg, HumanMessage) else \"AI\"\n",
        "                f.write(f\"{role}: {msg.content}\\n\")\n",
        "        print(\"Historique de conversation sauvegardé\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de la sauvegarde de l'historique: {str(e)}\")\n",
        "\n",
        "# Fonction pour charger l'historique des conversations\n",
        "def load_conversation_history():\n",
        "    try:\n",
        "        if os.path.exists(\"conversation_history.txt\"):\n",
        "            history = []\n",
        "            with open(\"conversation_history.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "                lines = f.readlines()\n",
        "                for line in lines:\n",
        "                    if line.startswith(\"Human: \"):\n",
        "                        history.append(HumanMessage(content=line[7:].strip()))\n",
        "                    elif line.startswith(\"AI: \"):\n",
        "                        history.append(AIMessage(content=line[4:].strip()))\n",
        "            return history\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors du chargement de l'historique: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Initialisation de la mémoire de conversation améliorée\n",
        "# Utilisation de ConversationBufferWindowMemory pour garder un nombre défini de messages\n",
        "memory = ConversationBufferWindowMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    input_key=\"question\",\n",
        "    output_key=\"answer\",\n",
        "    k=10  # Nombre de tours de conversation à conserver\n",
        ")\n",
        "\n",
        "# Chargement de l'historique précédent s'il existe\n",
        "previous_history = load_conversation_history()\n",
        "if previous_history:\n",
        "    for msg in previous_history:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            memory.chat_memory.add_user_message(msg.content)\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            memory.chat_memory.add_ai_message(msg.content)\n",
        "    print(f\"Historique chargé: {len(previous_history)} messages\")\n",
        "\n",
        "# Définition du template de prompt personnalisé avec référence explicite à l'historique\n",
        "qa_prompt = PromptTemplate(\n",
        "    template=(\n",
        "        \"Vous êtes un expert en soins et produits cosmétiques. \"\n",
        "        \"Utilisez les informations du contexte ci-dessous pour répondre avec précision et pertinence. \"\n",
        "        \"Adaptez vos conseils selon le type de peau, les préférences naturelles ou les besoins en maquillage.\\n\\n\"\n",
        "        \"Contexte: {context}\\n\"\n",
        "        \"Question du client: {question}\\n\\n\"\n",
        "        \"Réponse en tant que conseiller beauté:\"\n",
        "    ),\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Création de la chaîne de conversation avec récupération\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=groq_llm,\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    memory=memory,\n",
        "    return_source_documents=True,\n",
        "    combine_docs_chain_kwargs={\"prompt\": qa_prompt},\n",
        "    verbose=True  # Activer le mode verbeux pour le débogage\n",
        ")\n",
        "\n",
        "# Questions prédéfinies pour l'interface\n",
        "sample_questions = [\n",
        "    \"Quels soins recommandez-vous pour peau sèche ?\",\n",
        "    \"Quels sont les meilleurs produits de maquillage bio ?\",\n",
        "    \"Quel sérum utiliser pour les taches pigmentaires ?\",\n",
        "    \"Comment choisir une crème de nuit adaptée ?\",\n",
        "    \"Quels produits capillaires pour cheveux abîmés ?\"\n",
        "]\n",
        "\n",
        "# Fonction de réponse du chatbot\n",
        "def chatbot(question):\n",
        "    if not question.strip():\n",
        "        return \"Veuillez entrer une question.\"\n",
        "\n",
        "    try:\n",
        "        # Appel à la chaîne de conversation\n",
        "        response = qa_chain({\"question\": question})\n",
        "        answer = response['answer']\n",
        "\n",
        "        # Sauvegarde de l'historique après chaque échange\n",
        "        save_conversation_history(memory.chat_memory.messages)\n",
        "\n",
        "        # Extraction des sources\n",
        "        sources = \"\"\n",
        "        if 'source_documents' in response and response['source_documents']:\n",
        "            sources_list = []\n",
        "            for i, doc in enumerate(response['source_documents']):\n",
        "                try:\n",
        "                    page_info = f\"(Page {doc.metadata.get('page', 'N/A')})\"\n",
        "                    source_text = f\"**Document {i+1}:** {doc.page_content} {page_info}\"\n",
        "                    sources_list.append(source_text)\n",
        "                except Exception as e:\n",
        "                    sources_list.append(f\"**Document {i+1}:** [Erreur d'affichage: {str(e)}]\")\n",
        "\n",
        "            sources = \"\\n\\n\".join(sources_list)\n",
        "\n",
        "        # Formatage de la réponse avec Markdown\n",
        "        return f\"**Réponse :** {answer}\\n\\n**Sources :**\\n{sources}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Une erreur s'est produite: {str(e)}\"\n",
        "\n",
        "# Fonction pour afficher l'historique de conversation\n",
        "def show_history():\n",
        "    if hasattr(memory, 'chat_memory') and memory.chat_memory.messages:\n",
        "        try:\n",
        "            history_items = []\n",
        "            for msg in memory.chat_memory.messages:\n",
        "                role = \"Vous\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
        "                content = msg.content if msg.content else \"[Contenu vide]\"\n",
        "                history_items.append(f\"**{role}:** {content}\")\n",
        "\n",
        "            history = \"\\n\\n\".join(history_items)\n",
        "            return history\n",
        "        except Exception as e:\n",
        "            return f\"Erreur lors de l'affichage de l'historique: {str(e)}\"\n",
        "    return \"Aucun historique disponible.\"\n",
        "\n",
        "# Fonction pour effacer l'historique\n",
        "def clear_history():\n",
        "    try:\n",
        "        memory.clear()\n",
        "        # Supprimer également le fichier d'historique\n",
        "        if os.path.exists(\"conversation_history.txt\"):\n",
        "            os.remove(\"conversation_history.txt\")\n",
        "        return \"Historique effacé.\"\n",
        "    except Exception as e:\n",
        "        return f\"Erreur lors de l'effacement de l'historique: {str(e)}\"\n",
        "\n",
        "# Interface Gradio\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as interface:\n",
        "    gr.Markdown(\"## Assistant Mode - LangChain & Groq\\nPosez votre question sur les tendances mode 2025.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        question_input = gr.Textbox(placeholder=\"Posez votre question ici...\", label=\"Votre question\")\n",
        "        submit_btn = gr.Button(\"Envoyer\", variant=\"primary\")\n",
        "\n",
        "    output = gr.Markdown(label=\"Réponse\")\n",
        "\n",
        "    # Affichage de l'historique\n",
        "    with gr.Accordion(\"Historique de conversation\", open=False):\n",
        "        history_output = gr.Markdown()\n",
        "\n",
        "        with gr.Row():\n",
        "            show_history_btn = gr.Button(\"Afficher l'historique\")\n",
        "            clear_history_btn = gr.Button(\"Effacer l'historique\")\n",
        "\n",
        "        show_history_btn.click(fn=show_history, outputs=history_output)\n",
        "        clear_history_btn.click(fn=clear_history, outputs=history_output)\n",
        "\n",
        "    # Boutons pour les questions prédéfinies\n",
        "    gr.Markdown(\"### Questions Prédéfinies\")\n",
        "    with gr.Row():\n",
        "        for i in range(0, len(sample_questions), 2):\n",
        "            with gr.Column():\n",
        "                for j in range(i, min(i+2, len(sample_questions))):\n",
        "                    gr.Button(sample_questions[j]).click(\n",
        "                        fn=chatbot,\n",
        "                        inputs=gr.Textbox(value=sample_questions[j], visible=False),\n",
        "                        outputs=output\n",
        "                    )\n",
        "\n",
        "    # Lien entre l'entrée manuelle et le chatbot\n",
        "    submit_btn.click(chatbot, inputs=question_input, outputs=output)\n",
        "    question_input.submit(chatbot, inputs=question_input, outputs=output)\n",
        "\n",
        "# Lancement de l'interface\n",
        "interface.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}